{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import functools\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_model_params(model: nn.Module):\n",
    "    params = sum([p.numel() for p in model.parameters()])\n",
    "    print(f\"model parameters: {params:,}\")\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计CUDA显存\n",
    "def cuda_memory_decorator(f):    \n",
    "    @functools.wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        before = torch.cuda.memory_allocated() / (2**20)\n",
    "        result = f(*args, **kwargs)\n",
    "        after = torch.cuda.memory_allocated() / (2**20)\n",
    "        delta = after - before\n",
    "        print(f\"cuda memory_allocated: {after:.2f}M, delta: {delta:.2f}M\")\n",
    "\n",
    "        return result\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一次推理\n",
    "@cuda_memory_decorator\n",
    "def forward(model: nn.Module, x: torch.Tensor, no_grad: bool = False):\n",
    "    if no_grad:\n",
    "        context = torch.no_grad()\n",
    "    else:\n",
    "        context = contextlib.nullcontext()\n",
    "    \n",
    "    with context:\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return model(x)\n",
    "        elif isinstance(x, (list, tuple)):\n",
    "            return model(*x)\n",
    "        elif isinstance(x, dict):\n",
    "            return model(**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda_memory_decorator\n",
    "def forward_and_backward(model: nn.Module, x: torch.Tensor):\n",
    "    loss = model(x).sum()\n",
    "    loss.backward()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型和数据\n",
    "@cuda_memory_decorator\n",
    "def get_model_and_data():\n",
    "    x = torch.randn(1024, 1024, device=torch.cuda.current_device())\n",
    "\n",
    "    model = nn.Sequential(nn.Linear(1024, 1024, bias=False),\n",
    "                        nn.GELU(),\n",
    "                        nn.Linear(1024, 1024, bias=False),\n",
    "                        nn.GELU(),\n",
    "                        nn.Linear(1024, 1024, bias=False))\n",
    "    model.cuda()\n",
    "    \n",
    "    return model, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda memory_allocated: 16.00M, delta: 16.00M\n"
     ]
    }
   ],
   "source": [
    "model, x = get_model_and_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "多次前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda memory_allocated: 44.12M, delta: 28.12M\n",
      "cuda memory_allocated: 44.12M, delta: 20.00M\n",
      "cuda memory_allocated: 44.12M, delta: 20.00M\n",
      "cuda memory_allocated: 28.12M, delta: 4.00M\n"
     ]
    }
   ],
   "source": [
    "# forward pass, no_grad=False\n",
    "forward(model, x, no_grad=False)\n",
    "forward(model, x, no_grad=False)\n",
    "forward(model, x, no_grad=False)\n",
    "\n",
    "# forward pass，no_grad=TRUE\n",
    "_ = forward(model, x, no_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播显存占用分析，增加的显存占用来自中间变量和输出变量：\n",
    "- `no_grad=False`的前向传播，增加的显存占用来自于中间变量$4*4*1024*1024=16M$和输出变量4M，共20M。\n",
    "- **`no_grad=False`第一次前向传播比后续的前向传播，增加了8M显存，来源未知。**\n",
    "- `no_grad=True`的前向传播，增加的显存占用来自于输出变量4M。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播和反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda memory_allocated: 48.25M, delta: 20.13M\n"
     ]
    }
   ],
   "source": [
    "_ = forward_and_backward(model, x,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA显存占用分析，增加的显存占用来自梯度和输出：\n",
    "- 梯度，0.weight、2.weight和4.weight对应梯度大小分别为4M，共12M。\n",
    "- 输出，输出Tensor的大小为4M。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看模型梯度信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([1024, 1024])\n",
      "2.weight torch.Size([1024, 1024])\n",
      "4.weight torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(name, p.grad.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransformerEncode显存分析"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransformerEncoder包括多头自注意力MultiHeadAttention和多层感知机MLP两个子网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiHeadAttention显存分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda_memory_decorator\n",
    "def get_mha_model_data():\n",
    "    mha = nn.MultiheadAttention(1024, 1, bias=False)\n",
    "    mha.cuda()\n",
    "    x = torch.randn(2, 1024, 1024, device=torch.cuda.current_device())\n",
    "    \n",
    "    return mha, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda memory_allocated: 68.25M, delta: 24.00M\n"
     ]
    }
   ],
   "source": [
    "mha_model, mha_x = get_mha_model_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显存增加了24M，模型和数据分别占用16M和8M显存。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiHeadAttention模型参数信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_proj_weight torch.Size([3072, 1024])\n",
      "out_proj.weight torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "for name, p in mha_model.named_parameters():\n",
    "    print(name, p.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MHA前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda memory_allocated: 116.28M, delta: 48.03M\n",
      "cuda memory_allocated: 116.28M, delta: 48.03M\n",
      "cuda memory_allocated: 76.27M, delta: 8.02M\n"
     ]
    }
   ],
   "source": [
    "forward(mha_model, (mha_x, mha_x, mha_x))\n",
    "forward(mha_model, (mha_x, mha_x, mha_x))\n",
    "_ = forward(mha_model, (mha_x, mha_x, mha_x), no_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MHA前向传播显存占用分析， MHA计算公式：$softmax(\\frac {QK}{\\sqrt{d_k}})V$\n",
    "- `no_grad=False`，增加的显存来自中间变量和输出变量，根据MHA计算公式，中间变量占用显存大小$3*8M+8M+8M+8M=40M$，输出变量占用8M。\n",
    "- `no_grad=Fasle`，增加的显存来自输出变量8M。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda_memory_decorator\n",
    "def get_encoder_model_and_data():\n",
    "    encoder = nn.TransformerEncoderLayer(1024, 1, dim_feedforward=4096, dropout=0.)\n",
    "    encoder.cuda()\n",
    "    data = torch.randn(2, 1024, 1024, device=torch.cuda.current_device())\n",
    "\n",
    "    return encoder, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda memory_allocated: 132.32M, delta: 56.05M\n"
     ]
    }
   ],
   "source": [
    "encoder_model, encoder_x = get_encoder_model_and_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.0, inplace=False)\n",
      "  (dropout2): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder模型和数据占用显存：\n",
    "- 模型结构显存48M，其中MHA占用16M，MLP占用32M。\n",
    "- 数据占用显存8M。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda memory_allocated: 236.47M, delta: 104.16M\n",
      "cuda memory_allocated: 236.47M, delta: 104.16M\n",
      "cuda memory_allocated: 140.32M, delta: 8.00M\n"
     ]
    }
   ],
   "source": [
    "forward(encoder_model, encoder_x)\n",
    "forward(encoder_model, encoder_x)\n",
    "_ = forward(encoder_model, encoder_x, no_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder前向传播占用显存分析："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `no_grad=False`，前向传播占用显存104M，主要是中间变量和输出变量。\n",
    "    - MHA+Norm, 48M + 8M，共56M\n",
    "    - MLP+Norm, 40M + 8M，共48M\n",
    "- `no_grad=True`，前向传播占用显存是输出变量8M。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67d8830c3596a717380bd8b7db897e439a133fcbb627b9fcbfbdf242f06bd3d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
