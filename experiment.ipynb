{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dataclasses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_MAX_CONNECTIONS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class LMConfig:\n",
    "    num_layers: int\n",
    "    hidden_size: int\n",
    "    num_attention_head: int\n",
    "    seq_length: int = 1024\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    gpus_per_node: int\n",
    "\n",
    "    # train configuration\n",
    "    steps: int\n",
    "    batch_size: int\n",
    "    fp16: bool\n",
    "    \n",
    "    # model configuration\n",
    "    lm: LMConfig\n",
    "    \n",
    "    # parallel configuration\n",
    "    data_parallel_size: int = 1\n",
    "    tensor_parallel_size: int = 1\n",
    "    pipeline_parallel_size: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Metrics:\n",
    "    forward_backward_time: float\n",
    "    forward_time: float\n",
    "    backward_time: float\n",
    "    optimizer_time: float\n",
    "    communication_time: float\n",
    "    memory_size: float\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘图函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(data: pd.DataFrame):\n",
    "    ncols = len(data.columns)\n",
    "    x = list(data.index)\n",
    "    _, axs = plt.subplots(ncols=ncols, figsize=(16, 1))\n",
    "\n",
    "    for i, metric in enumerate(data.columns):\n",
    "        y = list(data[metric].values)\n",
    "        axs[i].barh(x, y)\n",
    "        axs[i].set_xlabel(metric)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 测试plot函数\n",
    "# data = {scheme: {\"compute\": random.randint(1, 10), \"optimizer\": random.randint(1, 10), \"memory\": random.randint(10,100), \"communication\": random.randint(100, 1000)} for scheme in [\"base\", \"fp16\"]}\n",
    "# data = pd.DataFrame(data).T\n",
    "# plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(cfg: Config):\n",
    "    cmd = f\"\"\"torchrun --nproc_per_node {cfg.gpus_per_node} --nnodes 1 --node_rank 0 pretrain_gpt.py \\\n",
    "    --distributed-backend nccl \\\n",
    "    --tensor-model-parallel-size {cfg.tensor_parallel_size} \\\n",
    "    --pipeline-model-parallel-size {cfg.pipeline_parallel_size} \\\n",
    "    --num-layers {cfg.lm.num_layers} \\\n",
    "    --hidden-size {cfg.lm.hidden_size} \\\n",
    "    --num-attention-heads {cfg.lm.num_attention_head} \\\n",
    "    --seq-length 1024 \\\n",
    "    --max-position-embeddings 1024 \\\n",
    "    --micro-batch-size {cfg.batch_size} \\\n",
    "    --global-batch-size {cfg.batch_size} \\\n",
    "    --train-iters {cfg.steps} \\\n",
    "    --lr 0.00015 \\\n",
    "    --lr-decay-iters 320000 \\\n",
    "    --lr-decay-style cosine \\\n",
    "    --min-lr 1.0e-5 \\\n",
    "    --weight-decay 1e-2 \\\n",
    "    --lr-warmup-fraction .01 \\\n",
    "    --clip-grad 1.0 \\\n",
    "    --log-interval 10 \\\n",
    "    --timing-log-level 2 \\\n",
    "    --data-path /data/datasets/gpt2/BookCorpusDataset_text_document \\\n",
    "    --vocab-file /data/datasets/gpt2/gpt2-vocab.json \\\n",
    "    --merge-file /data/datasets/gpt2/gpt2-merges.txt \\\n",
    "    --data-impl mmap \\\n",
    "    --split 949,50,1\"\"\"\n",
    "    \n",
    "    if cfg.fp16:\n",
    "        cmd += \" --fp16 \"\n",
    "\n",
    "    # 日志输出到train.log文件\n",
    "    cmd += \" > train.log 2>&1\"\n",
    "\n",
    "    print(cmd)\n",
    "    os.system(cmd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型参数量\n",
    "\n",
    "|model|params|num_layers|hidden_size|num_head|\n",
    "|---|----|---|----|---|\n",
    "|gpt2-xl|1558M|48|1600|25|\n",
    "|dollm-6B|6064M|48|3200|40|\n",
    "|dollm-13B|12848M|40|5120|40|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_xl_model = LMConfig(num_layers=48, hidden_size=1600, num_attention_head=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train configuration: \n",
      "{\n",
      "    \"gpus_per_node\": 1,\n",
      "    \"steps\": 50,\n",
      "    \"batch_size\": 1,\n",
      "    \"fp16\": true,\n",
      "    \"lm\": {\n",
      "        \"num_layers\": 48,\n",
      "        \"hidden_size\": 3200,\n",
      "        \"num_attention_head\": 25,\n",
      "        \"seq_length\": 1024\n",
      "    },\n",
      "    \"data_parallel_size\": 1,\n",
      "    \"tensor_parallel_size\": 1,\n",
      "    \"pipeline_parallel_size\": 1\n",
      "}\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... infer\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  encoder_num_layers .............................. 48\n",
      "  encoder_seq_length .............................. 1024\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  ffn_hidden_size ................................. 12800\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_e4m3 ........................................ False\n",
      "  fp8_hybrid ...................................... False\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 1\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 3200\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 25\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 48\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  output_bert_embeddings .......................... False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ 1\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_workdir ................................... None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 969, 30, 1\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 2\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... None\n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> initializing torch distributed ...\n",
      "> initialized tensor model parallel with size 1\n",
      "> initialized pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/workspace/Megatron-LM/megatron/data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/workspace/Megatron-LM/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "ninja: no work to do.\n",
      "ninja: no work to do.\n",
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /workspace/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /workspace/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /workspace/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module scaled_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /workspace/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /workspace/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_dense_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_dense_cuda...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "ninja: no work to do.\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 0.703 seconds\n",
      "time to initialize megatron (seconds): 1.929\n",
      "[after megatron is initialized] datetime: 2023-05-17 09:48:17 \n",
      "building GPT model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"pretrain_gpt.py\", line 118, in <module>\n",
      "    pretrain(train_valid_test_datasets_provider, model_provider,\n",
      "  File \"/workspace/Megatron-LM/megatron/training.py\", line 111, in pretrain\n",
      "    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(\n",
      "  File \"/workspace/Megatron-LM/megatron/training.py\", line 371, in setup_model_and_optimizer\n",
      "    model = get_model(model_provider_func, model_type)\n",
      "  File \"/workspace/Megatron-LM/megatron/training.py\", line 251, in get_model\n",
      "    model = model_provider_func(\n",
      "  File \"pretrain_gpt.py\", line 26, in model_provider\n",
      "    model = GPTModel(\n",
      "  File \"/workspace/Megatron-LM/megatron/model/gpt_model.py\", line 61, in __init__\n",
      "    self.language_model, self._language_model_key = get_language_model(\n",
      "  File \"/workspace/Megatron-LM/megatron/model/language_model.py\", line 67, in get_language_model\n",
      "    language_model = TransformerLanguageModel(\n",
      "  File \"/workspace/Megatron-LM/megatron/model/language_model.py\", line 347, in __init__\n",
      "    args.padded_vocab_size,\n",
      "AttributeError: 'Namespace' object has no attribute 'padded_vocab_size'\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 73576) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch==1.14.0a0+410ce96', 'console_scripts', 'torchrun')())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/run.py\", line 762, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/run.py\", line 753, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "pretrain_gpt.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-05-17_09:48:18\n",
      "  host      : 5c47a5fc6240\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 73576)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = Config(gpus_per_node=1, steps=50, batch_size=1, fp16=True, lm=gpt2_xl_model)\n",
    "print(f\"train configuration: \\n{json.dumps(dataclasses.asdict(cfg), indent=4)}\")\n",
    "\n",
    "train_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(cfg)\n",
    "# ret, metrics = parse_log(\"train.log\")\n",
    "# if ret:\n",
    "#     print(f\"metrics: \\n{json.dumps(dataclasses.asdict(metrics), indent=4)}\")\n",
    "# else:\n",
    "#     print(\"train failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "55a26945acae1dbe174fa8a7f2737f59bcc9ca988f8fc990f33e458e609cda8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
