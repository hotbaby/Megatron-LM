{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dataclasses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_MAX_CONNECTIONS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class LMConfig:\n",
    "    num_layers: int\n",
    "    hidden_size: int\n",
    "    num_attention_head: int\n",
    "    seq_length: int = 1024\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    gpus_per_node: int\n",
    "\n",
    "    # train configuration\n",
    "    steps: int\n",
    "    batch_size: int\n",
    "    fp16: bool\n",
    "    \n",
    "    # model configuration\n",
    "    lm: LMConfig\n",
    "    \n",
    "    # parallel configuration\n",
    "    data_parallel_size: int = 1\n",
    "    tensor_parallel_size: int = 1\n",
    "    pipeline_parallel_size: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Metrics:\n",
    "    forward_backward_time: float\n",
    "    forward_time: float\n",
    "    backward_time: float\n",
    "    optimizer_time: float\n",
    "    communication_time: float\n",
    "    memory_size: float\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘图函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(data: pd.DataFrame):\n",
    "    ncols = len(data.columns)\n",
    "    x = list(data.index)\n",
    "    _, axs = plt.subplots(ncols=ncols, figsize=(16, 1))\n",
    "\n",
    "    for i, metric in enumerate(data.columns):\n",
    "        y = list(data[metric].values)\n",
    "        axs[i].barh(x, y)\n",
    "        axs[i].set_xlabel(metric)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 测试plot函数\n",
    "# data = {scheme: {\"compute\": random.randint(1, 10), \"optimizer\": random.randint(1, 10), \"memory\": random.randint(10,100), \"communication\": random.randint(100, 1000)} for scheme in [\"base\", \"fp16\"]}\n",
    "# data = pd.DataFrame(data).T\n",
    "# plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(cfg: Config):\n",
    "    cmd = f\"\"\"torchrun --nproc_per_node {cfg.gpus_per_node} --nnodes 1 --node_rank 0 pretrain_gpt.py \\\n",
    "    --distributed-backend nccl \\\n",
    "    --tensor-model-parallel-size {cfg.tensor_parallel_size} \\\n",
    "    --pipeline-model-parallel-size {cfg.pipeline_parallel_size} \\\n",
    "    --num-layers {cfg.lm.num_layers} \\\n",
    "    --hidden-size {cfg.lm.hidden_size} \\\n",
    "    --num-attention-heads {cfg.lm.num_attention_head} \\\n",
    "    --seq-length 1024 \\\n",
    "    --max-position-embeddings 1024 \\\n",
    "    --micro-batch-size {cfg.batch_size} \\\n",
    "    --global-batch-size {cfg.batch_size} \\\n",
    "    --train-iters {cfg.steps} \\\n",
    "    --lr 0.00015 \\\n",
    "    --lr-decay-iters 320000 \\\n",
    "    --lr-decay-style cosine \\\n",
    "    --min-lr 1.0e-5 \\\n",
    "    --weight-decay 1e-2 \\\n",
    "    --lr-warmup-fraction .01 \\\n",
    "    --clip-grad 1.0 \\\n",
    "    --log-interval 10 \\\n",
    "    --timing-log-level 2 \\\n",
    "    --data-path /data/datasets/gpt2/BookCorpusDataset_text_document \\\n",
    "    --vocab-file /data/datasets/gpt2/gpt2-vocab.json \\\n",
    "    --merge-file /data/datasets/gpt2/gpt2-merges.txt \\\n",
    "    --data-impl mmap \\\n",
    "    --split 949,50,1\"\"\"\n",
    "    \n",
    "    if cfg.fp16:\n",
    "        cmd += \" --fp16 \"\n",
    "\n",
    "    # 日志输出到train.log文件\n",
    "    cmd += \" > train.log 2>&1\"\n",
    "\n",
    "    # print(cmd)\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型参数量\n",
    "\n",
    "|model|params|num_layers|hidden_size|num_head|\n",
    "|---|----|---|----|---|\n",
    "|gpt2-xl|1558M|48|1600|25|\n",
    "|dollm-6B|6064M|48|3200|40|\n",
    "|dollm-13B|12848M|40|5120|40|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_xl_model = LMConfig(num_layers=48, hidden_size=1600, num_attention_head=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train configuration: \n",
      "{\n",
      "    \"gpus_per_node\": 1,\n",
      "    \"steps\": 50,\n",
      "    \"batch_size\": 1,\n",
      "    \"fp16\": true,\n",
      "    \"lm\": {\n",
      "        \"num_layers\": 48,\n",
      "        \"hidden_size\": 1600,\n",
      "        \"num_attention_head\": 25,\n",
      "        \"seq_length\": 1024\n",
      "    },\n",
      "    \"data_parallel_size\": 1,\n",
      "    \"tensor_parallel_size\": 1,\n",
      "    \"pipeline_parallel_size\": 1\n",
      "}\n",
      "torchrun --nproc_per_node 1 --nnodes 1 --node_rank 0 pretrain_gpt.py     --distributed-backend nccl     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --num-layers 48     --hidden-size 1600     --num-attention-heads 25     --seq-length 1024     --max-position-embeddings 1024     --micro-batch-size 1     --global-batch-size 1     --train-iters 50     --lr 0.00015     --lr-decay-iters 320000     --lr-decay-style cosine     --min-lr 1.0e-5     --weight-decay 1e-2     --lr-warmup-fraction .01     --clip-grad 1.0     --log-interval 10     --timing-log-level 2     --data-path /data/datasets/gpt2/BookCorpusDataset_text_document     --vocab-file /data/datasets/gpt2/gpt2-vocab.json     --merge-file /data/datasets/gpt2/gpt2-merges.txt     --data-impl mmap     --split 949,50,1 --fp16  > train.log 2>&1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = Config(gpus_per_node=1, steps=50, batch_size=1, fp16=True, lm=gpt2_xl_model)\n",
    "print(f\"train configuration: \\n{json.dumps(dataclasses.asdict(cfg), indent=4)}\")\n",
    "\n",
    "train_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(cfg)\n",
    "# ret, metrics = parse_log(\"train.log\")\n",
    "# if ret:\n",
    "#     print(f\"metrics: \\n{json.dumps(dataclasses.asdict(metrics), indent=4)}\")\n",
    "# else:\n",
    "#     print(\"train failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Mar  2 2023, 03:21:46) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "55a26945acae1dbe174fa8a7f2737f59bcc9ca988f8fc990f33e458e609cda8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
